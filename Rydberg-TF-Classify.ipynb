{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "os.chdir('/home/users/stariq/Codes/cudaq-qrc')\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from dynamics import *\n",
    "import re\n",
    "import math\n",
    "from optread import *\n",
    "from scipy.stats import binom\n",
    "#set latex font for plots\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= utilities for fair-size baselines =================================\n",
    "def set_seed(seed):\n",
    "    import random, numpy as np, torch\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def pick_emb_dim(budget, n_feat, nhead=4, nlayers=2, ff_mult=4, tol=.10):\n",
    "    \"\"\"\n",
    "    Return the largest emb_dim (multiple of nhead) whose 2-layer Transformer\n",
    "    fits within `budget·(1+tol)` params.\n",
    "    \"\"\"\n",
    "    def n_params(d):\n",
    "        mha = 3*d*d*nlayers            # Q,K,V\n",
    "        out = d*d   *nlayers           # W_O\n",
    "        ff  = 2*d*ff_mult*d*nlayers    # W1,W2\n",
    "        ln  = 2*d*nlayers              # layer norms\n",
    "        bias= d*(ff_mult*2*nlayers + nlayers + 1)  # FF + attn + regressor\n",
    "        pos = d*(n_feat+1)             # pos + CLS\n",
    "        return mha+out+ff+ln+bias+pos\n",
    "    d = max(nhead, nhead*((budget//n_feat)//nhead))\n",
    "    while n_params(d) > budget*(1+tol):\n",
    "        d -= nhead\n",
    "        if d < nhead: raise ValueError(\"budget too small\")\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: X_train (300, 4) X_val (100, 4) X_test (100, 4) y_train (300,) y_val (100,) y_test (100,)\n"
     ]
    }
   ],
   "source": [
    "N_classes = 4\n",
    "N_features = 4\n",
    "N_samples  = 500\n",
    "\n",
    "# 1) Generate a multiclass classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples    = N_samples,\n",
    "    n_features   = N_features,\n",
    "    n_repeated   = 0,\n",
    "    n_classes    = N_classes,\n",
    "    n_clusters_per_class = 1,\n",
    "    class_sep    = 1.0,\n",
    "    flip_y       = 0.01,\n",
    "    random_state = 402\n",
    ")\n",
    "\n",
    "# 2) Normalize features to [0, 1]\n",
    "scaler_X      = MinMaxScaler(feature_range=(0, 1))\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "\n",
    "# 3) Split: 60% train, 40% temp (test+val)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_normalized, y,\n",
    "    test_size   = 0.4,\n",
    "    shuffle     = True,\n",
    "    random_state= 402\n",
    ")\n",
    "\n",
    "# 4) Split the 40% temp equally into validation (20%) and test (20%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size   = 0.5,\n",
    "    shuffle     = True,\n",
    "    random_state= 402\n",
    ")\n",
    "\n",
    "print(\"Shapes:\",\n",
    "      \"X_train\", X_train.shape,\n",
    "      \"X_val\",   X_val.shape,\n",
    "      \"X_test\",  X_test.shape,\n",
    "      \"y_train\", y_train.shape,\n",
    "      \"y_val\",   y_val.shape,\n",
    "      \"y_test\",  y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from dynamics import embeddings_multi_gpu_auto as emb_mgpu\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_t  = torch.tensor(y_test,  dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 2\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#gpu count\n",
    "gpu_count = torch.cuda.device_count()\n",
    "print(\"Number of GPUs available:\", gpu_count)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------- #\n",
    "# Random-Fourier baseline in “dim-tunable” form                        #\n",
    "# --------------------------------------------------------------------- #\n",
    "class RandomFourierClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    φ(x) = √(2/D) cos(x·W + b)  →  linear → logits over n_classes\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, out_dim: int, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.out_dim   = out_dim\n",
    "        self.register_buffer(\"W\", torch.randn(in_dim, out_dim) * 0.1)\n",
    "        self.register_buffer(\"b\", torch.randn(out_dim) * 2 * math.pi)\n",
    "        # now outputs n_classes logits\n",
    "        self.linear = nn.Linear(out_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):  # x : (B, F)\n",
    "        phi    = torch.cos(x @ self.W + self.b) * math.sqrt(2 / self.out_dim)\n",
    "        logits = self.linear(phi)               # (B, n_classes)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "# Constructor that _search_dim_multiple_ can call                       #\n",
    "# --------------------------------------------------------------------- #\n",
    "def make_rff(dim):\n",
    "    return RandomFourierClassifier(\n",
    "        in_dim    = X_train_t.shape[1],\n",
    "        out_dim   = dim,\n",
    "        n_classes = N_classes\n",
    "    ).to(device)\n",
    "\n",
    "    \n",
    "# --------------------------------------------------------------------- #\n",
    "# Trainable-parameter count helper (buffers are ignored automatically)  #\n",
    "# --------------------------------------------------------------------- #\n",
    "def n_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MGTransformerClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_features: int,\n",
    "                 emb_dim: int = 64,\n",
    "                 nhead: int = 4,\n",
    "                 nhid: int = 512,\n",
    "                 nlayers: int = 4,\n",
    "                 dropout: float = 0.1,\n",
    "                 n_classes: int = 4):\n",
    "        super().__init__()\n",
    "        self.feat_proj = nn.Linear(1, emb_dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
    "        self.pos_emb   = nn.Parameter(torch.randn(1, n_features+1, emb_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_dim, nhead=nhead,\n",
    "            dim_feedforward=nhid,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, nlayers)\n",
    "        self.classifier  = nn.Linear(emb_dim, n_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, F = x.shape\n",
    "        h = self.feat_proj(x.unsqueeze(-1))      # (B, F, E)\n",
    "        cls = self.cls_token.expand(B, -1, -1)   # (B, 1, E)\n",
    "        h   = torch.cat([cls, h], dim=1)         # (B, F+1, E)\n",
    "        h  += self.pos_emb                       # add pos emb\n",
    "        h   = self.transformer(h)                # (B, F+1, E)\n",
    "        cls_emb = h[:,0]                         # (B, E)\n",
    "        return self.classifier(cls_emb)          # (B, n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, loader_tr, *,\n",
    "                     epochs       = 100,\n",
    "                     lr           = 1e-3,\n",
    "                     weight_decay = 1e-4,\n",
    "                     patience     = 10,\n",
    "                     desc         = \"train\",\n",
    "                     early_stop   = True):\n",
    "    \"\"\"\n",
    "    Train a classifier (supports both (x,y) and (x, reservoir, y) batches).\n",
    "    Returns list of training cross-entropy losses per epoch.\n",
    "    \"\"\"\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_state, best_loss, wait = None, float(\"inf\"), 0\n",
    "    train_losses = []\n",
    "\n",
    "    bar = tqdm(range(1, epochs+1), desc=f\"{desc}: CE=∞\", leave=False)\n",
    "    for ep in bar:\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for batch in loader_tr:\n",
    "            # unpack batch\n",
    "            if len(batch) == 3:\n",
    "                xb, reservoir, yb = batch\n",
    "                reservoir = reservoir.to(device)\n",
    "            else:\n",
    "                xb, yb = batch\n",
    "                reservoir = None\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            # forward\n",
    "            logits = model(xb, reservoir) if reservoir is not None else model(xb)\n",
    "            loss   = F.cross_entropy(logits, yb)\n",
    "\n",
    "            # step\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            running += loss.item() * xb.size(0)\n",
    "\n",
    "        ce = running / len(loader_tr.dataset)\n",
    "        train_losses.append(ce)\n",
    "        bar.set_description(f\"{desc}: CE={ce:.4f}\")\n",
    "\n",
    "        # early stopping on training loss\n",
    "        if ce < best_loss:\n",
    "            best_loss, best_state, wait = ce, model.state_dict().copy(), 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if early_stop and wait >= patience:\n",
    "                break\n",
    "\n",
    "    bar.close()\n",
    "    model.load_state_dict(best_state)\n",
    "    return train_losses\n",
    "\n",
    "\n",
    "def predict_classifier(model, loader):\n",
    "    \"\"\"\n",
    "    Inference supporting both (x,y) and (x, reservoir, y) batches.\n",
    "    Returns (preds, trues) as LongTensors.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if len(batch) == 3:\n",
    "                xb, reservoir, yb = batch\n",
    "                reservoir = reservoir.to(device)\n",
    "            else:\n",
    "                xb, yb = batch\n",
    "                reservoir = None\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb, reservoir) if reservoir is not None else model(xb)\n",
    "            preds.append(logits.argmax(dim=1).cpu())\n",
    "            trues.append(yb)\n",
    "    return torch.cat(preds), torch.cat(trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——————— Prepare DataLoaders ———————\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "test_ds  = TensorDataset(X_test_t,  y_test_t)\n",
    "loader_tr = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "loader_te = DataLoader(test_ds,  batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:\n",
      "Device 0: Tesla V100-SXM2-16GB\n",
      "Device 1: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "#print alll available devices\n",
    "print(\"Available devices:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Use GPUs 0, 1, 2, and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ n=4: loaded cached embeddings\n"
     ]
    }
   ],
   "source": [
    "all_train_emb, all_test_emb = {}, {}\n",
    "\n",
    "for n in range(4, 5):          # 2 … 10 qubits\n",
    "    tr_emb, te_emb = get_embeddings_for_nsites(\n",
    "        nsites      = n,\n",
    "        X_train     = X_train,     # your current arrays\n",
    "        X_test      = X_test,\n",
    "        dataset_tag = f\"{N_classes}-{N_features}feat-classify\"   # or \"MackeyGlass\", etc.\n",
    "    )\n",
    "    all_train_emb[n] = tr_emb\n",
    "    all_test_emb[n]  = te_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_flat.shape: (300, 180)\n",
      "test_flat.shape:  (100, 180)\n"
     ]
    }
   ],
   "source": [
    "#testing for n = 2\n",
    "nsites = n\n",
    "readouts = generate_readouts(nsites)\n",
    "train_embeddings = all_train_emb[nsites]\n",
    "test_embeddings = all_test_emb[nsites]\n",
    "T_res = 6\n",
    "R = len(readouts)\n",
    "print(\"train_flat.shape:\", train_embeddings.shape)\n",
    "print(\"test_flat.shape: \", test_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3) reshape into 3D\n",
    "# N, D = train_embeddings.shape\n",
    "# N_test, D_test = test_embeddings.shape\n",
    "# train3 = train_embeddings.reshape(N, T_res, R)\n",
    "# test3  = test_embeddings.reshape(N_test, T_res, R)\n",
    "\n",
    "\n",
    "# # inject a realistic cocktail of errors\n",
    "# train_noisy3 = noisy_embeddings(train3, gaussian_sigma=0.2,\n",
    "#                                 multiplicative_sigma=0.01,\n",
    "#                                 shots=800,\n",
    "#                                 T2=3.0, dt=0.5)\n",
    "# test_noisy3  = noisy_embeddings(test3,  gaussian_sigma=0.2,\n",
    "#                                 multiplicative_sigma=0.01,\n",
    "#                                 shots=800,\n",
    "#                                 T2=3.0, dt=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "# reshape to (batch, T, R)\n",
    "R = len(readouts)\n",
    "T = train_embeddings.shape[1] // R\n",
    "res_tr = train_embeddings.reshape(-1, T, R)\n",
    "res_te = test_embeddings.reshape(-1, T, R)\n",
    "\n",
    "# res_tr =selected_train_embeddings.reshape(-1, T, R)\n",
    "# res_te =selected_test_embeddings.reshape(-1, T, R)\n",
    "# to tensors\n",
    "res_tr_t = torch.tensor(res_tr, dtype=torch.float32)\n",
    "res_te_t = torch.tensor(res_te, dtype=torch.float32)\n",
    "ld_qr_tr = DataLoader(\n",
    "    TensorDataset(X_train_t, res_tr_t, y_train_t),\n",
    "    batch_size=32, shuffle=True\n",
    ")\n",
    "ld_qr_te = DataLoader(\n",
    "    TensorDataset(X_test_t,  res_te_t,  y_test_t),\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #count parameters\n",
    "# print(\"Reservoir-Attention Model params:\", count_parameters(model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QRCrossAttnLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-attention from the reservoir onto *all* tokens.\n",
    "    reservoir : (B,T,R)\n",
    "    tokens    : (B,F+1,E)\n",
    "    x_feat_emb: (B,E)  – mean-pooled scalar embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, res_dim, ff_mult=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.score_proj = nn.Linear(res_dim, 1)      # (T,R) → (T,1)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb_dim, ff_mult * emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ff_mult * emb_dim, emb_dim),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, tokens, reservoir, x_feat_emb):\n",
    "        # 1) attention weights from reservoir\n",
    "        attn    = torch.softmax(self.score_proj(reservoir), dim=1)   # (B,T,1)\n",
    "\n",
    "        # 2) value vector = feature embedding (no extra proj)\n",
    "        context = (attn * x_feat_emb.unsqueeze(1)).sum(1)            # (B,E)\n",
    "\n",
    "        # 3) add to *every* token, then FFN\n",
    "        tokens  = self.norm1(tokens + context.unsqueeze(1))\n",
    "        tokens  = self.norm2(tokens + self.ff(tokens))\n",
    "        return tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MGTransformerQRClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_features: int,\n",
    "                 res_dim:     int,\n",
    "                 emb_dim:     int,\n",
    "                 n_classes:   int = 4,\n",
    "                 ff_mult:     int = 4,\n",
    "                 dropout:     float = 0.0):\n",
    "        super().__init__()\n",
    "        self.feat_proj = nn.Linear(n_features, emb_dim)\n",
    "        self.layer     = QRCrossAttnLayer(emb_dim, res_dim,\n",
    "                                          ff_mult=ff_mult,\n",
    "                                          dropout=dropout)\n",
    "        self.classifier= nn.Linear(emb_dim, n_classes)\n",
    "\n",
    "    def forward(self, x, reservoir):\n",
    "        feat_emb = self.feat_proj(x)              # (B, E)\n",
    "        tokens   = feat_emb.unsqueeze(1)          # (B, 1, E)\n",
    "        tokens   = self.layer(tokens, reservoir, feat_emb)\n",
    "        cls_emb  = tokens.squeeze(1)              # (B, E)\n",
    "        return self.classifier(cls_emb)           # (B, n_classes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum-Transformer :  1403 params  (E=12)\n",
      "Transformer         :  2032 params  (E=12)\n",
      "Random Fourier      :  1624 params  (D=405)\n"
     ]
    }
   ],
   "source": [
    "# ════════════════════════════════════════════════════════════════════\n",
    "# 1️⃣  parameter counter & emb-dim search\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "def n_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def search_dim_multiple(\n",
    "        ctor, *, target,  nhead=4,  low=8, high=512, tol=0.05, **kw):\n",
    "    \"\"\"\n",
    "    Binary-search emb_dim (multiple of nhead) so that\n",
    "    |#params - target| / target  ≤ tol\n",
    "    \"\"\"\n",
    "    low  = math.ceil(low  / nhead) * nhead\n",
    "    high = math.floor(high / nhead) * nhead\n",
    "    best, best_p = None, float(\"inf\")\n",
    "\n",
    "    while low <= high:\n",
    "        mid   = (low + high) // (2*nhead) * nhead  # rounded to multiple\n",
    "        model = ctor(mid, **kw);  p = n_params(model)\n",
    "\n",
    "        if abs(p-target) <= tol*target:   # hit!\n",
    "            return model, p\n",
    "        if p < target: low  = mid + nhead\n",
    "        else:          high = mid - nhead\n",
    "\n",
    "        if abs(p-target) < abs(best_p-target):\n",
    "            best, best_p = model, p\n",
    "    return best, best_p                        # closest found\n",
    "\n",
    "\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "# 2️⃣  constructors with **identical MLP width 4×E** and depth = 1\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "def make_transformer(E):\n",
    "    return MGTransformerClassifier(\n",
    "        n_features = X_train_t.shape[1],\n",
    "        emb_dim    = E,\n",
    "        nhead      = 4,\n",
    "        nhid       = 4*E,\n",
    "        nlayers    = 1,\n",
    "        dropout    = 0.0,\n",
    "        n_classes  = N_classes\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "def make_qt(E):\n",
    "    return MGTransformerQRClassifier(\n",
    "        n_features= X_train_t.shape[1],\n",
    "        res_dim    = R,\n",
    "        emb_dim    = E,\n",
    "        n_classes  = N_classes,\n",
    "        ff_mult    = 4,\n",
    "        dropout    = 0.0\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "def make_rff(D):\n",
    "    return RandomFourierClassifier(\n",
    "        in_dim    = X_train_t.shape[1],\n",
    "        out_dim   = D,\n",
    "        n_classes = N_classes\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "# 3️⃣  pick dimensions so every model ≈ TARGET params\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "TARGET = 1_600        # you can change this in one place\n",
    "\n",
    "model_tf , p_tf  = search_dim_multiple(make_transformer,\n",
    "                                       target=TARGET, nhead=4)\n",
    "model_rff, p_rff = search_dim_multiple(make_rff,\n",
    "                                       target=TARGET, nhead=1,\n",
    "                                       low=32, high=12_000)\n",
    "\n",
    "model_qt , p_qt  = search_dim_multiple(make_qt,\n",
    "                                       target=TARGET, nhead=4)\n",
    "print(f\"Quantum-Transformer : {p_qt :>5} params  (E={model_qt.feat_proj.out_features})\")\n",
    "\n",
    "print(f\"Transformer         : {p_tf :>5} params  (E={model_tf.feat_proj.out_features})\")\n",
    "\n",
    "print(f\"Random Fourier      : {p_rff:>5} params  (D={model_rff.out_dim})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee2d2bf22c145eead5e1e7bed09d108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train: CE=∞:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4f914c54ea4906921a7915e6c7ef12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train: CE=∞:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47304da42775499ead0689bf4cefc147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train: CE=∞:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m loss_tf  \u001b[38;5;241m=\u001b[39m train_classifier(model_tf,  loader_tr, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m      3\u001b[0m loss_rff \u001b[38;5;241m=\u001b[39m train_classifier(model_rff, loader_tr, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m loss_qt  \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_qt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mld_qr_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 2) Evaluate on test set\u001b[39;00m\n\u001b[1;32m      7\u001b[0m pred_tf,  true_tf  \u001b[38;5;241m=\u001b[39m predict_classifier(model_tf,  loader_te)\n",
      "Cell \u001b[0;32mIn[40], line 20\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[0;34m(model, loader_tr, epochs, lr, weight_decay, patience, desc, early_stop)\u001b[0m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     19\u001b[0m running \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xb, yb \u001b[38;5;129;01min\u001b[39;00m loader_tr:\n\u001b[1;32m     21\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mto(device), yb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(xb)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# 1) Train each model\n",
    "loss_tf  = train_classifier(model_tf,  loader_tr, epochs=100, lr=1e-3)\n",
    "loss_rff = train_classifier(model_rff, loader_tr, epochs=100, lr=1e-3)\n",
    "loss_qt  = train_classifier(model_qt,  ld_qr_tr, epochs=100, lr=1e-3)\n",
    "\n",
    "# 2) Evaluate on test set\n",
    "pred_tf,  true_tf  = predict_classifier(model_tf,  loader_te)\n",
    "pred_rff, true_rff = predict_classifier(model_rff, loader_te)\n",
    "pred_qt,  true_qt  = predict_classifier(model_qt,  ld_qr_te)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print(\"Transformer Test Acc:\", accuracy_score(true_tf,  pred_tf))\n",
    "print(\"RFF         Test Acc:\", accuracy_score(true_rff, pred_rff))\n",
    "print(\"Quantum-Transformer Test Acc:\", accuracy_score(true_qt,  pred_qt))\n",
    "\n",
    "print(\"\\nClassification report (Transformer):\")\n",
    "print(classification_report(true_tf, pred_tf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum-Transformer :  1466 params  (E=12)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb793f6fd3ad48b0af0dd1f85c5223d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QT: loss=∞:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cudaq)",
   "language": "python",
   "name": "cudaq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
